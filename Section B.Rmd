---
title: "Scientific Computing and Empirical Methods Summative Assessment Section B"
author: "Joe Anderson"
date: "05/12/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE}
# Imports
library(Stat2Data)
library(tidyverse)
```

## B.1
### a.

$$p_0 = P(Sensor | ¬Person)$$
$$p_1 = P(Sensor | Person)$$
$$q = P(Person)$$
$$\phi = P(Person | Sensor)$$

By Bayes' theorem:

$$\phi = \frac{P(Sensor | Person)P(Person)}{P(S)}$$

By the definition of conditional probability:

$$\begin{align*}
P(Sensor) & = P(Sensor \bigcap Person) + P(Sensor \bigcap Person)\\
& = P(Sensor | Person)P(Person) + P(Sensor | ¬Person)P(¬Person)\\
& = qp_1 + (1-q)p_0
\end{align*}$$

Substituting into the Bayes' equation:

$$\phi = \frac{qp_1}{qp_1 + (1-q)p_0}$$
```{r}
c_prob_person_given_alarm <- function(p0, p1, q){
  phi <- q * p1 / (q * p1 + (1 - q) * p0)
  return (phi)
}
```

### b.

```{r}
p0 <- 0.05
p1 <- 0.95

phi <- c_prob_person_given_alarm(0.05, 0.95, 0.1)
print(paste("Phi: ", phi))
```
### c.

```{r}
qs <- seq(0, 1, 0.01)

prob_by_qs <- data.frame(qs) %>%
  mutate(prob = c_prob_person_given_alarm(p0, p1, qs))

ggplot(data = prob_by_qs, aes(x = qs, y = prob)) + geom_line() + theme_bw() + xlab("q probability") + ylab("Phi probability")

```


## B.2

### a.

$$\begin{equation}
    p(x) =
    \left\{
        \begin{array}{cc}
                 1 - \alpha - \beta - \gamma  & \mathrm{if\ } x = 0 \\
                 \alpha   & \mathrm{if\ } x = 1 \\
                  \beta & \mathrm{if\ } x = 2 \\
                  \gamma & \mathrm{if\ } x = 5\\
                  0& \mathrm{otherwise}\\
                  
        \end{array} 
    \right.
\end{equation}$$

### b.

$$\begin{align*}
E(X) & = (1 - \alpha - \beta - \gamma) \times 0 + \alpha \times 1 + \beta \times 2 +\gamma \times 5\\
& = \alpha + 2\beta + 5\gamma
\end{align*}
$$

### c.

$$Var(X) = E(X^2) - E(X)^2 $$

$$E(X^2) = \alpha + 4\beta + 25\gamma$$

$$\begin{align*}
Var(X) & =  \alpha + 4\beta + 25\gamma - (\alpha + 2\beta + 5\gamma)^2\\
& = \alpha + 4\beta + 25\gamma - (\alpha^2 + 4\alpha\beta + 10\alpha\gamma +  4\beta^2 + 20\beta\gamma + 25\gamma^2 )
\end{align*}
$$

### d.

$$\begin{align*}
E(\bar{X}) & = \frac{1}{n}\sum^n_{i=1}E(X_i)\\
& = \frac{1}{n}\sum^n_{i=1}(\alpha + 2\beta + 5\gamma)\\
& = \frac{n}{n}(\alpha + 2\beta + 5\gamma)\\
& = \alpha + 2\beta + 5\gamma
\end{align*}
$$

### e.


$$\begin{align*}
Var(\bar{X}) & = Var(\frac{1}{n}\sum^n_{i=1}X_i)\\
& = \frac{1}{n^2}\sum^n_{i=1}Var(X_i)\\
& = \frac{n}{n^2}(\alpha + 4\beta + 25\gamma - (\alpha + 2\beta + 5\gamma)^2) \\
& = \frac{1}{n}(\alpha + 4\beta + 25\gamma - (\alpha + 2\beta + 5\gamma)^2)
\end{align*}$$

### f.

```{r}
sample_X_0125 <- function(alpha, beta, gamma, n){
  
  sample_X<-data.frame(U=runif(n))%>%
    mutate(X=case_when((0 <= U)&(U < alpha) ~ 1,
    (alpha <= U) & (U < alpha + beta) ~ 2,
    (alpha + beta <= U) & (U < alpha + beta + gamma) ~ 5,
    (alpha + beta + gamma <= U) & (U <=1 ) ~ 0))%>%
    pull(X)
  
return(sample_X)
}
```

### g.
```{r}
alpha <- 0.1
beta <- 0.2
gamma <- 0.3
n <- 100000

set.seed(0)
sample <- sample_X_0125(alpha, beta, gamma, n)

sample_mean <- mean(sample)
sample_var <- var(sample)

print(paste("Sample mean: ", sample_mean))
print(paste("Sample variance: ", sample_var))

```

Check if this is as expected by using expressions from earlier:

```{r}
exp_mean <- alpha + 2 * beta + 5 * gamma
exp_var <-  (alpha + 4 * beta + 25 * gamma) - (alpha + 2 * beta + 5 * gamma)^2

print(paste("Expected sample mean: ", exp_mean))
print(paste("Expected sample variance: ", exp_var))
```
 The values are each very close, which is a we would expect when performing this number of trials. By the law of large numbers, we would expect mean and variance to tend to 2 and 4.4 respectively.
 
### h.

```{r}
alpha <- 0.1
beta <- 0.2
gamma <- 0.3
n <- 100

set.seed(0)
trials_df <- data.frame(trial = seq(1,10000)) %>%
  mutate(sample = map(.x = trial, ~sample_X_0125(alpha, beta, gamma, n))) %>%
  mutate(mean = map_dbl(.x = sample, ~mean(.x)))
```

### i.

```{r}
ggplot(data = trials_df, aes(x = mean)) + geom_histogram(binwidth = 0.02) + xlab("Mean") + ylab("Count")
```

### j.

```{r}
means <- trials_df %>%
  pull(mean)

exp_mean <- mean(means)
exp_var <- var(means)

exp_mean <- format(round(exp_mean, 4), nsmall = 4) 
exp_var <- format(round(exp_var, 4), nsmall = 4) 

print(paste("Expectation X bar: ", exp_mean))
print(paste("Variance X bar ", exp_var))
```

### k.

```{r}
exp_mean = as.numeric(exp_mean)
exp_var = as.numeric(exp_var)

xs <- seq(exp_mean - 4 * sqrt(exp_var), exp_mean + 4 * sqrt(exp_var), 0.0001)
gauss <- 200 * dnorm(xs, mean = exp_mean, sd = sqrt(exp_var))

colors <- c("Histogram" = "black", "Gaussian density" = "blue")
fills <- c("Histogram" = "grey", "Gaussian density" = "grey")

ggplot() + 
  theme_bw() + 
  geom_line(data = data.frame(xs, gauss), aes(x = xs, y = gauss,  color = "Gaussian density"), size = 2) +
  geom_histogram(data = trials_df, aes(x = mean, fill = "Histogram", color = "Histogram"), binwidth = 0.02) + 
  xlab("Mean") + 
  ylab("Count") + 
  scale_color_manual(name = "", values = colors) +
  scale_fill_manual(name = "", values = fills) 

```


### l.

We observe a clear relationship between the two plots. This is as a result of the central limit theorem. Each observation of the sample mean within the simulation experiment is a random variable with expectation 2 and variance of approximately 0.04, from the earlier calculations and empirical methods. 

Letting $W \sim \mathcal{N}(2, 0.04)$, so that $\bar{W} = \frac{W - 2}{\sqrt{0.04}}$ is a standard Gaussian of the form $\bar{W}\sim \mathcal{N}(0,1)$.

The central limit theorem states:

$$\begin{align*}
\lim_{n\to \infty}\mathbb{P} \Bigg\{ \sqrt\frac{n}{0.04}(\frac{1}{n}\sum_{i=1}^nX_i - 2) \le x \Bigg\} & =
\mathbb{P}(\bar{W} \le x) \\
& = \mathbb{P}(W < 2 + x\sqrt{0.04})\\
& = \mathbb{P}(W < 2 + 0.2x)
\end{align*}$$

In this case we have n = 10,000, so we see it converging to a fair approximation of this Gaussian. We have to remember the factor of 200, as the pdf of a Gaussian gives probabilities in the range 0 to 1, whereas our experiment gives a count of each mean size for 10, 000 trials.

## B.3

### a.
By parts:
$$\begin{align*}
E(X) & = \int^\infty_{-\infty} xp_\lambda(x)dx \\
& = \int^\infty_0 \lambda x e^{-\lambda x}dx \\
& = [-xe^{-\lambda x}]^\infty_0 +  \int^\infty_0 e^{-\lambda x}dx \\
& = [-\frac{1}{\lambda}e^{-\lambda x}]^\infty_0\\
& = \frac{1}{\lambda}
\end{align*}$$


Again by parts:

$$\begin{align*}
E(X^2) & = \int^\infty_{-\infty} x^2p_\lambda(x)dx \\
& = \int^\infty_0 \lambda x^2 e^{-\lambda x}dx \\
& = [-x^2e^{-\lambda x}]^\infty_0 +  2\int^\infty_0 xe^{-\lambda x}dx \\
& = \frac{2}{\lambda}\int^\infty_0 \lambda xe^{-\lambda x}dx \\
& = \frac{2}{\lambda}E(X) \\
& = \frac{2}{\lambda^2} \\
Var(X) & = \frac{2}{\lambda^2} - \frac{1}{\lambda}^2 = \frac{1}{\lambda}^2
\end{align*}$$


## b.

$$F_\lambda(x) = \int^x_{-\infty}p_\lambda(t)dt\\$$

$$\begin{equation}
    =
    \left\{
        \begin{array}{cc}
                 0  & \mathrm{if\ } x \le 0 \\
                 \int^x_0\lambda e^{-\lambda t}   & \mathrm{if\ } x \gt 0
                  
        \end{array} 
    \right.
\end{equation}$$

$$\int^x_0\lambda e^{-\lambda t} = 1 - e^ {-\lambda x}$$

$$\begin{equation}
    =
    \left\{
        \begin{array}{cc}
                 0  & \mathrm{if\ } x \le 0 \\
                 1 - e^ {-\lambda x}   & \mathrm{if\ } x \gt 0
                  
        \end{array} 
    \right.
\end{equation}$$

### c.

$$\ell(\lambda) = \prod^n_{i=1} f_\lambda(X_i) = \lambda^ne^{-n\lambda\bar(X)} \\
\log \ell(\lambda) = n\log(\lambda) - n\lambda\bar{X}\\
\frac{\partial}{\partial\lambda}\log\ell(\lambda) = \frac{n}{\lambda} - n \bar{X}
$$
Max when $\frac{\partial}{\partial\lambda}\log\ell(\lambda) = 0$:

$$\frac{n}{\lambda} - n \bar{X} = 0\\
\hat{\lambda}_{\mathrm{MLE}} = \frac{1}{\bar{X}}$$

### d.

```{r}
set.seed(0)
lambda_0 <- 0.01
num_trials_per_sample_size <- 100
min_sample_size <- 5
max_sample_size <- 1000
sample_size_inc <- 5

exp_mse_simulation_df <- crossing(trial = seq(num_trials_per_sample_size),
                                  sample_size = seq(min_sample_size, max_sample_size, sample_size_inc)) %>%
  mutate(sample = pmap(.l = list(trial, sample_size), ~rexp(.y, lambda_0))) %>%
  mutate(lambda_mle = 1/map_dbl(.x = sample, .f = mean)) %>%
  group_by(sample_size) %>%
  summarise(mse = mean((lambda_mle - lambda_0)^2))

ggplot(data = exp_mse_simulation_df, aes(x = sample_size, y = mse)) + theme_bw() + geom_smooth() + xlab("Sample size")+ ylab("Mean square error")
```

### e.
```{r}
folder_name<-"Data"
file_name<-"bird_data_EMATM0061"
bird_data<-read.csv(paste0(folder_name,"\\",file_name,".csv"))

bird_data <- bird_data %>%
  mutate(time_diffs = lead(Time) - Time) 

time_diffs = bird_data %>% pull(time_diffs)

lambda_MLE <- 1 / mean(time_diffs, na.rm = TRUE)
print(paste("Maximum likelihood estimaror for rate parameter: ", lambda_MLE))

```

### f.

```{r}
exp_confidence_interval <- function(sample,confidence_level){
  alpha <- 1 - confidence_level
  z_alpha <- qnorm(1 - alpha / 2)
  n <- length(sample)
  mn <- mean(sample, na.rm = TRUE)
  ci_l <- (1 / mn) * (1 - z_alpha / sqrt(n))
  ci_u <- (1 / mn) * (1 + z_alpha / sqrt(n))
  return(c(ci_l,ci_u))
}

exp_confidence_interval(time_diffs, 0.95)
```























